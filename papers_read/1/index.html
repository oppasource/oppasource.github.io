<!DOCTYPE HTML>
<html>

<head>
  <title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>

  <script
    src="https://code.jquery.com/jquery-3.3.1.js"
    
    crossorigin="anonymous">
</script>
<script> 
$(function(){
  $("#header1").load("/header1.html"); 
  $("#footer1").load("/footer1.html"); 
});
</script> 

  <meta name="description" content="research papers read" />
  <meta name="keywords" content="website keywords, website keywords" />
  <meta http-equiv="content-type" content="text/html; charset=windows-1252" />
  <link rel="stylesheet" type="text/css" href="../style/style.css" title="style" />
</head>

<body>
  <div id="main">
    <div id="header1"></div>
    <div id="content_header"></div>
    <div id="site_content">
      <div id="content">
        <!-- insert the page content here -->
        <h1><strong>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</strong></h1>
        <h2>"What" part of the paper</h2> 
        <p>Paraphrase Detection using a method which incorporates not just single words but also phrase level semantics.</p>

        <h2>"How" part of the paper</h2>
        <p>First RAE is used to get a similarity matrix, this simililarity matrix is passed to dynamic pooling to get a fixed length matrix which can be used to train any classifier for paraphrase identification.</p>

        <span class="center"><img src="8.png" alt="something wrong" width="100%" height="100%"/></span>

        <p>To incorporate word level and phrase level semantics to identify paraphrases, <strong><i>Recursive Autoencoder</i> (RAE) </strong>are used. For a given sentence we find binary parse tree. We take embeddings of each word, for a given parse of two words to a parent node(childern will always be two because of the binary parse tree) we take embeddings of both children, concatnate them and pass it through a single layer neural network to get embedding of the parent node. We repeate this for each binary parse with same weights of neural network everytime.</p>

        <p>To train the weights, we try to re-generate the children representation from the parent node representation. We calculate the error between original representation and re-generated representation and train the weights to reduce this error.</p>

        <span class="center"><img src="4.png" alt="something wrong" width="100%" height="100%"/></span>

        We find the parent node representation with child representation as follows:

        <span class="center"><img src="1.png" alt="something wrong" /></span>

        <p>Note that the leaves of the tree will be word embeddings. Then we use another decoding linear layer with different weights to re-generate the children from the parent representation. And we compute the error to trains the encoding and decoding weights as follows:</p>

        <span class="center"><img src="2.png" alt="something wrong" width="100%" height="100%"/></span>

        <p>Total error of whole tree T is given as:</p>

        <span class="center"><img src="3.png" alt="something wrong" /></span>

        <hr><br/>

        <p>There is also another <i><strong>Unfolding Recursive Autoencoder</strong></i> where the encoding scheme is the same but for decoding step it tries to reconstruct the entire spanned subtree underneath each node. Changes with respect to above equations is as shown:</p>

        <span class="center"><img src="5.png" alt="something wrong" /></span>
        <span class="center"><img src="6.png" alt="something wrong" /></span>
        <span class="center"><img src="7.png" alt="something wrong" /></span>

        <p>We then compute Euclidean distances between all word and phrase vectors of the two sentences from the RAE tree to get the similarity matrix. This way we incorporate similarity between all words and phrases between two sentences.</p>

        <span class="right"><img src="9.png" alt="something wrong" width="60%" height="60%"/></span>

        <hr><br/>

        <p><i><strong>Dynamic Pooling</strong></i> is used to transform variable length similarity matrix to fixed length. Lets say we have similarity matrix of size <i>m x n</i> and we want to convert it to <i>A x A</i>. Then we first partition similarity matrix into <i>A</i> roughly equal parts and we take minimum value from each of the rectangular region of this grid. We will then have a fixed size matrix to train any classifier.</p> 



        <br/><br/><br/>
        <hr><br/>

        <h2>Reference</h2>
        <a href="https://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</a>

        <br/><br/>




      </div>
    </div>

    <div id="footer1"></div>
    
  </div>
</body>
</html>
